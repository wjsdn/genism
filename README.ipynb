{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genism\n",
    "2201210498 尹雯婧\n",
    "## 1. genism安装\n",
    "```text\n",
    "pip install genism\n",
    "```\n",
    "```text\n",
    "Successfully installed Cython-0.29.28 gensim-4.2.0 smart-open-6.2.0\n",
    "```\n",
    "## 2. 使用方法\n",
    "### (1) 准备训练语料\n",
    "假设列表 documents 代表语料库，每一句话代表一个文档，documents 中有9个元素，也就是说该语料库由9个文档组成。\n",
    "```python\n",
    "from gensim import corpora\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]\n",
    "```\n",
    "### (2) 预处理\n",
    "分词（tokenize）、去除停用词（stopwords）和在语料中只出现一次的词。\n",
    "处理语料的方式有很多，这里只通过空格（whitespace）去分词，把每个词变为小写，最后去除一些常用的词和只出现一次的词。\n",
    "```python\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "for document in documents]\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "               for text in texts]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(texts)\n",
    "```\n",
    "输出结果为：\n",
    "```text\n",
    "[['human', 'interface', 'computer'],\n",
    " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    " ['eps', 'user', 'interface', 'system'],\n",
    " ['system', 'human', 'system', 'eps'],\n",
    " ['user', 'response', 'time'],\n",
    " ['trees'],\n",
    " ['graph', 'trees'],\n",
    " ['graph', 'minors', 'trees'],\n",
    " ['graph', 'minors', 'survey']]\n",
    "```\n",
    "### (3) 文本向量化\n",
    "这里使用词袋模型（bag-of-words）来提取文档特征。该模型通过计算每个词在文档中出现的频率，然后将这些频率组成一个向量，从而将文档向量化。\n",
    "首先我们需要用语料库训练一个词典，词典包含所有在语料库中出现的单词。\n",
    "```python\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('./deerwester.dict')\n",
    "print(dictionary)\n",
    "print(dictionary.token2id)\n",
    "```\n",
    "输出结果为：\n",
    "```text\n",
    "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n",
    "```\n",
    "```text\n",
    "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n",
    "```\n",
    "上面已经构建了单词词典，我们可以通过该词典用词袋模型将其他的文本向量化。\n",
    "```python\n",
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)\n",
    "```\n",
    "输出结果为：\n",
    "```text\n",
    "[(0, 1), (1, 1)]\n",
    "```\n",
    "假设新文本是\"Human computer interaction\"，则输出向量为[(0, 1), (1, 1)]。\n",
    "其中，(0, 1)中的\"0\"表示 computer 在词典中的 id 为 0，\"1\"表示 Human 在该文档中出现了1次。\n",
    "同理，(1, 1)表示 Human 在词典中的 id 为 1，出现次数为 1。\n",
    "interaction 不在词典中，所以没有任何对应输出。\n",
    "通过以下操作，我们能够看到我们这次得到的语料库：\n",
    "```python\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('/out/deerwester.mm', corpus) # 存入硬盘，以备后需\n",
    "print(corpus)\n",
    "```\n",
    "输出结果为：\n",
    "```text\n",
    "[(0, 1), (1, 1), (2, 1)]\n",
    "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
    "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
    "[(1, 1), (5, 2), (8, 1)]\n",
    "[(3, 1), (6, 1), (7, 1)]\n",
    "[(9, 1)]\n",
    "[(9, 1), (10, 1)]\n",
    "[(9, 1), (10, 1), (11, 1)]\n",
    "[(4, 1), (10, 1), (11, 1)]\n",
    "```\n",
    "### (4) 语料库流\n",
    "在以上的训练过程中，一整个语料库作为一个 Python List 存在了内存中。如果语料库很大，这样的存储方式对内存很不友好。\n",
    "我们可以一次取出一个文档，这样同一时间只有一个文档在内存中。\n",
    "```python\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('mycorpus.txt'):\n",
    "            yield dictionary.doc2bow(line.lower().split())\n",
    "corpus_memory_friendly = MyCorpus()# 没有将corpus加载到内存中\n",
    "print(corpus_memory_friendly)\n",
    "\n",
    "for vector in corpus_memory_friendly:  # 每次加载一个向量放入内存\n",
    "    print(vector)\n",
    "```\n",
    "输出结果为：\n",
    "```text\n",
    "<__main__.MyCorpus object at 0x000001F4F23FDE50>\n",
    "```\n",
    "```text\n",
    "[(0, 1), (1, 1), (2, 1)]\n",
    "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
    "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
    "[(1, 1), (5, 2), (8, 1)]\n",
    "[(3, 1), (6, 1), (7, 1)]\n",
    "[(9, 1)]\n",
    "[(9, 1), (10, 1)]\n",
    "[(9, 1), (10, 1), (11, 1)]\n",
    "[(4, 1), (10, 1), (11, 1)]\n",
    "```\n",
    "相似地，为了构造 dictionary 我们也不必将全部文档读入内存：\n",
    "```python\n",
    "from gensim import corpora\n",
    "from six import iteritems\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "#初步构建所有单词的词典\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt') )\n",
    "#去出停用词,stop_ids表示停用词在dictionary中的id\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "#只出现一次的单词id\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq ==1]\n",
    "#根据stop_ids与once_ids清洗dictionary\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "# 去除清洗后的空位\n",
    "dictionary.compactify()\n",
    "print(dictionary)\n",
    "```\n",
    "输出结果为：\n",
    "```text\n",
    "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n",
    "```\n",
    "### (5) 主题向量的变换\n",
    "Genism的核心之一就是对文本向量的转换。通过挖掘语料中隐藏的语义结构特征，我们最终可以变换出一个简洁高效的文本向量。\n",
    "在Gensim中，每一个向量变换的操作都对应着一个模型，例如上面提到的对应着词袋模型的doc2bow变换。\n",
    "每一个模型又都是一个标准的Python对象。\n",
    "下面以TF-IDF模型为例，介绍Gensim模型的一般使用方法。\n",
    "\n",
    "首先是模型对象的初始化。通常，Gensim 模型接受一段训练语料作为初始化的参数。显然，越复杂的模型需要配置的参数越多。\n",
    "```python\n",
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "```\n",
    "其中，corpus是一个返回bow向量的迭代器。这两行代码将完成对corpus中出现的每一个特征的IDF值的统计工作。\n",
    "\n",
    "接下来，我们可以调用这个模型将任意一段语料转化成TFIDF向量。\n",
    "需要注意的是，这里的bow向量必须与训练语料的bow向量共享同一个特征字典（即共享同一个向量空间）。\n",
    "```python\n",
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow])\n",
    "```\n",
    "输出结果为：\n",
    "```text\n",
    "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n",
    "```\n",
    "Gensim内置了多种主题模型的向量变换，包括LDA，LSI，RP，HDP等。\n",
    "这些模型通常以bow向量或tfidf向量的语料为输入，生成相应的主题向量。所有的模型都支持流式计算。\n",
    "### (6) 文档相似度计算\n",
    "在得到每一篇文档对应的主题向量后，我们就可以计算文档之间的相似度，进而完成如文本聚类、信息检索之类的任务。Gensim提供了这一类任务的API接口。\n",
    "\n",
    "以信息检索为例。对于一篇待检索的query，我们的目标是从文本集合中检索出主题相似度最高的文档。\n",
    "\n",
    "首先，我们需要将待检索的query和文本放在同一个向量空间里进行表达：\n",
    "```python\n",
    "# 构造LSI模型并将待检索的query和文本转化为LSI主题向量\n",
    "# 转换之前的corpus和query均是BOW向量\n",
    "lsi_model = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "documents = lsi_model[corpus]\n",
    "query_vec = lsi_model[query]\n",
    "```\n",
    "接下来，我们用待检索的文档向量初始化一个相似度计算的对象：\n",
    "```python\n",
    "index = similarities.MatrixSimilarity(documents)\n",
    "```\n",
    "最后，我们借助index对象计算任意一段query和所有文档的相似度：\n",
    "```python\n",
    "sims = index[query_vec]\n",
    "```\n",
    "\n",
    "#### 参考资料\n",
    "[Gensim: Topic modelling for humans](https://radimrehurek.com/gensim/)  \n",
    "[Gensim 中文文档](https://gensim.apachecn.org/#/)  \n",
    "[Genism 入门教程](https://www.cnblogs.com/iloveai/p/gensim_tutorial.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
